from typing import Dict, List

import matplotlib.pyplot as plt
import numpy as np
from sklearn.calibration import calibration_curve
from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve


def plot_metrics(
    log_dir: str,
    targets: List,
    predicts: List,
    classes: List,
) -> Dict:
    classes = np.array(classes)
    targets = np.array(targets)
    predicts = np.array(predicts)
    plot_auroc(log_dir, predicts, targets, classes)
    plot_pr_curve(log_dir, predicts, targets, classes)
    plot_confusion_matrix(log_dir, predicts, targets, classes)
    eval_matrix(log_dir, predicts, targets, classes)


def plot_auroc(log_dir, predicts, targets, classes):
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(len(classes)):
        fpr[i], tpr[i], _ = roc_curve(targets == i, predicts == i)
        roc_auc[i] = roc_auc_score(targets == i, predicts == i)

    # Plot the ROC curves for each class
    fig, ax = plt.subplots()
    for i in range(len(classes)):
        ax.plot(fpr[i], tpr[i], label='ROC Curve for Class %s (AUC = %0.2f)' % (
            classes[i], roc_auc[i]))
    ax.plot([0, 1], [0, 1], 'r--')
    ax.set(xlabel='False Positive Rate', ylabel='True Positive Rate',
           title='ROC Curves for All Classes')
    ax.legend()
    plt.savefig(f'{log_dir}/auroc.png')
    plt.close()


def plot_pr_curve(log_dir, predicts, targets, classes):
    precision = dict()
    recall = dict()
    pr_auc = dict()
    for i in range(len(classes)):
        precision[i], recall[i], _ = precision_recall_curve(
            targets == i, predicts == i)
        pr_auc[i] = roc_auc_score(targets == i, predicts == i)

    # Plot the precision-recall curves for each class
    fig, ax = plt.subplots()
    for i in range(len(classes)):
        ax.plot(recall[i], precision[i],
                label='Precision-Recall Curve for Class %s (AUC = %0.2f)' % (classes[i], pr_auc[i]))
    ax.set(xlabel='Recall', ylabel='Precision',
           title='Precision-Recall Curves for All Classes')
    ax.legend()
    plt.savefig(f'{log_dir}/pr_curve.png')
    plt.close()
    plt.close(fig)


def plot_confusion_matrix(log_dir, predicts, targets, classes):
    # Compute the confusion matrix
    cm = confusion_matrix(targets, predicts)

    # Plot the confusion matrix
    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           xlabel='Predicted label', ylabel='True label')
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], 'd'),
                    ha="center", va="center",
                    color="white" if cm[i, j] > cm.max() / 2. else "black")
    fig.colorbar(im)
    plt.savefig(f'{log_dir}/confusion_matrix.png')
    plt.close()


def plot_acc(log_dir, acc_train, acc_test):

    plt.plot(acc_train, label='train_acc')
    plt.plot(acc_test, label='test_acc')
    plt.xlabel('epoch')
    plt.ylabel('acc')
    plt.title('Accuracy of Train and Test value')
    plt.legend()
    plt.savefig(f'{log_dir}/acc.png')
    plt.close()


def plot_loss(log_dir, loss_train, loss_test):

    plt.plot(loss_train, label='train_loss')
    plt.plot(loss_test, label='test_loss')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.title('Loss of Train and Test value')
    plt.legend()
    plt.savefig(f'{log_dir}/loss.png')
    plt.close()


def eval_matrix(log_dir, predicts, targets, classes):
    # Compute the accuracy, F1 score, precision, and recall for each class
    f1 = f1_score(targets, predicts, average=None)
    precision = precision_score(
        targets, predicts, average=None, zero_division=0)
    recall = recall_score(targets, predicts, average=None)

    # Create a table with the metrics for each class
    table = [['Class', 'F1_Score', 'Precision', 'Recall']]
    for i in range(len(np.unique(classes))):
        row = [classes[i],
               '{:.2f}'.format(f1[i]), '{:.2f}'.format(precision[i]),
               '{:.2f}'.format(recall[i])]
        table.append(row)

    # Write the table to a file
    with open(f'{log_dir}/metrics.log', 'w') as f:
        for row in table:
            f.write('{:<10}{:>10}{:>10}{:>10}\n'.format(*row))